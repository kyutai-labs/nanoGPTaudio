{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "from model import GPT, GPTConfig\n",
    "from tokenizer import (\n",
    "    CharTokenizer,\n",
    "    CodecTokenizer,\n",
    "    MuLawTokenizer,\n",
    "    TiktokenTokenizer,\n",
    "    Tokenizer,\n",
    "    audio_tokenizer_from_name,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = (\n",
    "    \"resume\"  # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    ")\n",
    "# out_dir = \"models/lm_librilight_0926_100659\" # mimi 8 rvq\n",
    "out_dir = \"models/lm_librilight_0827_110754\"  # mimi 32 rvq\n",
    "start = \"\\n\"  # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 5  # number of samples to draw\n",
    "max_new_tokens = 400  # number of tokens generated in each sample\n",
    "temperature = 0.8\n",
    "# retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "top_k = int(1e6)\n",
    "seed = 1337\n",
    "device = \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")  # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False  # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # for later use in torch.autocast\n",
    "ptdtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "}[dtype]\n",
    "\n",
    "model: GPT\n",
    "\n",
    "# model\n",
    "if init_from == \"resume\":\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint[\"model_args\"])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint[\"model\"]\n",
    "    unwanted_prefix = \"_orig_mod.\"\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith(\"gpt2\"):\n",
    "    # init from a given GPT-2 model\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unknown init_from value: {init_from}. Expected 'resume' or 'gpt2-<size>'.\"\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model)  # requires PyTorch 2.0 (optional)\n",
    "\n",
    "\n",
    "meta_path = os.path.join(\"data\", checkpoint[\"config\"][\"dataset\"], \"meta.json\")\n",
    "print(f\"Loading meta from {meta_path}...\")\n",
    "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "modality: Literal[\"text\", \"audio\"] = meta.get(\"modality\", \"text\")\n",
    "tokenizer: Tokenizer\n",
    "\n",
    "if modality == \"text\":\n",
    "    tokenizer = CharTokenizer(meta)\n",
    "    # TODO: when to load GPT-2 tokenizer?\n",
    "elif modality == \"audio\":\n",
    "    tokenizer = audio_tokenizer_from_name(meta[\"tokenizer\"])\n",
    "else:\n",
    "    raise ValueError(f\"Unknown modality: {modality}. Expected 'text' or 'audio'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the beginning of the prompt\n",
    "if start.startswith(\"FILE:\"):\n",
    "    if modality == \"text\":\n",
    "        with open(start[len(\"FILE:\") :], \"r\", encoding=\"utf-8\") as f:\n",
    "            start_data = f.read()\n",
    "    elif modality == \"audio\":\n",
    "        # Read audio file using librosa\n",
    "        start_data, _ = librosa.load(start[len(\"FILE:\") :], sr=meta[\"sample_rate\"])\n",
    "else:\n",
    "    if modality == \"text\":\n",
    "        start_data = start\n",
    "    elif modality == \"audio\":\n",
    "        assert start == \"\\n\", \"Specifying `start` as a literal doesn't work for audio\"\n",
    "        start_data = np.array([0.0])\n",
    "\n",
    "start_ids = tokenizer.encode(start_data)\n",
    "\n",
    "samples_dir = Path(out_dir) / \"samples\"\n",
    "file_prefix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_audio_file(file: Path | str) -> np.ndarray:\n",
    "    audio, sr = librosa.load(file, mono=True)\n",
    "    audio = librosa.resample(audio, orig_sr=sr, target_sr=meta[\"sample_rate\"])\n",
    "    # audio_mu_law = (librosa.mu_compress(audio, mu=255) + 128).astype(np.uint8)\n",
    "    return audio\n",
    "\n",
    "\n",
    "example_audio = load_audio_file(\n",
    "    \"/lustre/scwpod02/client/kyutai/datasets/librilight_segmented/train/4667/7513/historyofchurch_06_maccaffrey_64kb_0003.flac\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import MimiTokenizer\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "start_ids = tokenizer.encode(example_audio[: meta[\"sample_rate\"] * 6])\n",
    "print(start_ids.shape)\n",
    "start_ids = start_ids.to(\"cuda\")\n",
    "Audio(tokenizer.decode(start_ids), rate=meta[\"sample_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.auto\n",
    "\n",
    "# run generation\n",
    "tokens_per_timestep = meta[\"tokens_per_timestep\"]\n",
    "\n",
    "assert start_ids.ndim == 1, f\"Expected 1D result from encode(), got {start_ids.shape=}\"\n",
    "x = torch.tensor(\n",
    "    start_ids[: tokens_per_timestep * 1 + 1], dtype=torch.long, device=device\n",
    ")\n",
    "x = x.repeat(num_samples, 1)\n",
    "print(x.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ctx = (\n",
    "        nullcontext()\n",
    "        if device_type == \"cpu\"\n",
    "        else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "    )\n",
    "    with ctx:\n",
    "        for t in tqdm.auto.trange(len(start_ids) // tokens_per_timestep - 2):\n",
    "            y = model.generate(\n",
    "                x,\n",
    "                max_new_tokens=tokens_per_timestep - 1,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                progress_bar=False,\n",
    "            )\n",
    "            # samples_dir.mkdir(parents=True, exist_ok=True)\n",
    "            # for k in range(num_samples):\n",
    "            #     if modality == \"text\":\n",
    "            #         print(tokenizer.decode(y[k].tolist()))\n",
    "            #         print(\"---------------\")\n",
    "            #     elif modality == \"audio\":\n",
    "            #         audio = tokenizer.decode(y[k])\n",
    "\n",
    "            if x.shape[1] + 8 >= len(start_ids):\n",
    "                break\n",
    "\n",
    "            x = torch.cat((y, start_ids[y.shape[1]].repeat(num_samples, 1)), dim=1)\n",
    "            # x = torch.cat((x, y), dim=1)\n",
    "            # print(y.shape, x.shape)\n",
    "            # x = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(num_samples):\n",
    "    audio = tokenizer.decode(y[k])\n",
    "    display(Audio(audio, rate=meta[\"sample_rate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(audio, rate=meta[\"sample_rate\"]))\n",
    "display(Audio(audio, rate=meta[\"sample_rate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
